{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9d93d47",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import pandas as pd\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ebbc2615",
   "metadata": {},
   "outputs": [],
   "source": [
    "# set path for data import\n",
    "notebook_dir = os.path.dirname(os.path.abspath(\"__file__\"))\n",
    "data_path = os.path.join(notebook_dir, '..', '..', 'confidential_data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8f2c1be",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(f'{data_path}/merged_data.csv', index_col=0, parse_dates=True)\n",
    "data.dropna(inplace=True)\n",
    "# delete rows where volume is zero\n",
    "data = data[data['sp500_volume'] != 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "771d525b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(data.head())\n",
    "print(data.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37a7439",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create variable Log Returns and further\n",
    "data[\"DailyReturn\"] = np.log(data[\"sp500_close\"]).diff()\n",
    "data['Sign_1d'] = (data['DailyReturn'] > 0).astype(int)\n",
    "data['Volume_change'] = np.log(data['sp500_volume']).diff().shift(1)\n",
    "#data['Trading_range'] = np.log(data['sp500_high'] / data['sp500_low']).shift(1)\n",
    "\n",
    "data = data.iloc[1:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2812433e",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"DailyReturn\"].describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f6163c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(data[\"DailyReturn\"], label='Daily Log Returns', alpha=0.7)\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75236a8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot the distribution of daily returns\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.hist(data[\"DailyReturn\"], bins=100, density=True, alpha=0.6, color='g')\n",
    "plt.title('Distribution of Daily Log Returns')\n",
    "plt.xlabel('Daily Log Return')\n",
    "plt.ylabel('Density')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "66b071d1",
   "metadata": {},
   "source": [
    "NOTE: If I use log returns, then there is a zero asset return (mean=0.0002). This would make the use of volatility as predictor for sign return described in Christofferson & Diebold (2006) not applicable to my project."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4674c013",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check for normality of returns\n",
    "jb_stat, jb_p = stats.jarque_bera(data[\"DailyReturn\"].dropna())\n",
    "print(f\"Jarque-Bera test statistic: {np.round(jb_stat, 2)}, p-value: {np.round(jb_p, 4)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a74b448",
   "metadata": {},
   "source": [
    "Hence, the Daily Returns do not follow a normal distribution and sign forecastibility with zero mean asset return but asymmetric distribution from [Christofferson et al.](https://economics.sas.upenn.edu/pier/working-paper/2006/direction-change-forecasts-based-conditional-variance-skewness-and-kurtosis) (2007) could be applied. By the descriptive statistics, it already becomes apparent that there is excess kurtosis, since the minimum return is roughly 22 st.dev. away from the mean (when returns go back to 1980). The probability for this to happen is so small, that this return would be even unlikely if we get a return every nanosecond from the start of the universe. Since Pr(Z>22)=2.44x10^-107 for a st.normal distr. Whereas, the universe exists for roughly 4,35x10^26=13,800,000,000x365x24x3600x10^9 nanoseconds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81a5e653",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get annualized realized volatility\n",
    "data['Realized_Vol_20d'] = data['DailyReturn'].rolling(window=20).std() * np.sqrt(252)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1acfc880",
   "metadata": {},
   "outputs": [],
   "source": [
    "# print nan values per column\n",
    "print(data.isna().sum())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d146dc62",
   "metadata": {},
   "source": [
    "TODO: Download Bond data from bloomberg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "644a2f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add lagged returns as features\n",
    "data['Return_Lag1'] = data['DailyReturn'].shift(1)\n",
    "data['oil_price_return'] = np.log(data['oil_close']).diff()\n",
    "data['vix_return'] = np.log(data['vix']).diff()\n",
    "data['sign_lag1'] = data['Sign_1d'].shift(1)\n",
    "\n",
    "data = data.dropna()\n",
    "\n",
    "# Feature matrix\n",
    "features = np.column_stack([\n",
    "    data['Realized_Vol_20d'].values,\n",
    "    data['Return_Lag1'].values,\n",
    "    data['Volume_change'].values,\n",
    "    #data['Trading_range'].values,\n",
    "    data['fed_fund_rate'].values,\n",
    "    data['treasury_3mo'].values,\n",
    "    data['treasury_10yr'].values,\n",
    "    data['treasury_30yr'].values,\n",
    "    data['corp_bond_rate_aaa'].values,\n",
    "    data['oil_price_return'].values,\n",
    "    data['vix'].values,\n",
    "    data['vix_return'].values,\n",
    "    data['sign_lag1'].values\n",
    "])\n",
    "\n",
    "input_variables = features.shape[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d1e47c1",
   "metadata": {},
   "source": [
    "# Deep Learning for Sign Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e7492c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "import random\n",
    "\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e9bd06e",
   "metadata": {},
   "source": [
    "## Create loss function that maximizes classification accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99fb4142",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MaximumUtilityLoss(nn.Module):\n",
    "    \"\"\"\n",
    "    Differentiable approximation of the Elliott-Lieli (2013) Maximum Utility estimator.\n",
    "    Optimizes correct classification rather than probability calibration.\n",
    "    \"\"\"\n",
    "    def __init__(self, c=0.0, sharpness=10.0, w_pos=1.0, w_neg=1.0):\n",
    "        super().__init__()\n",
    "        self.c = c\n",
    "        self.sharpness = sharpness\n",
    "        self.w_pos = w_pos\n",
    "        self.w_neg = w_neg\n",
    "\n",
    "    def forward(self, logits, y):\n",
    "        y_sign = 2 * y - 1 # -1 or 1\n",
    "        pred = torch.tanh(self.sharpness * (logits - self.c))  # get a sign approximation\n",
    "        b = torch.where(y > 0.5, self.w_pos, self.w_neg)\n",
    "        loss = 1-torch.mean(b * y_sign * pred)\n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e22b5dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "NUMBER_OF_LAGS = 100  # number of lagged observations\n",
    "HIDDEN_DIM = 64\n",
    "NUM_LAYERS = 4\n",
    "DROPOUT = 0.0\n",
    "NUMBER_OF_EPOCHS = 50\n",
    "CLASSIFICATION_THRESHOLD = 0.5\n",
    "LR = 3e-4\n",
    "LR_STEP_SIZE = 10       # after how many epochs to decrease LR\n",
    "LR_GAMMA = 0.5          # factor to decrease LR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b2d0ec1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create lagged feature matrix for LSTM\n",
    "def create_lagged_matrix_multifeature(X, window):\n",
    "    out = []\n",
    "    for i in range(window, len(X)):\n",
    "        out.append(X[i-window:i, :])\n",
    "    return np.array(out)\n",
    "\n",
    "X_lagged = create_lagged_matrix_multifeature(features, NUMBER_OF_LAGS)\n",
    "y_target = data['Sign_1d'].values[NUMBER_OF_LAGS:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55862638",
   "metadata": {},
   "source": [
    "## Define LSTM Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6159de9",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleLSTM(nn.Module):\n",
    "    def __init__(self, input_dim=1, hidden_dim=16, num_layers=1, dropout=0.0):\n",
    "        super(SimpleLSTM, self).__init__()\n",
    "        self.lstm = nn.LSTM(input_dim, hidden_dim, num_layers, batch_first=True, dropout=0.0)\n",
    "        self.fc = nn.Linear(hidden_dim, 1)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "    def forward(self, x):\n",
    "        out, _ = self.lstm(x)\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f21eacaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "dates_lagged = data.index[NUMBER_OF_LAGS:] \n",
    "years = dates_lagged.year\n",
    "unique_years = np.unique(years).astype(int)\n",
    "min_year = int(unique_years.min())\n",
    "max_year = int(unique_years.max())\n",
    "\n",
    "# require at least this many trading observations for a year to be considered \"full\"\n",
    "MIN_DAYS_PER_YEAR = 200\n",
    "\n",
    "# find first test_year such that test_year and previous 3 years each have >= MIN_DAYS_PER_YEAR samples\n",
    "first_valid_test_year = None\n",
    "for cand in range(min_year + 3, max_year + 1):\n",
    "    train_years = [cand - 3, cand - 2, cand - 1]\n",
    "    counts = {y: np.sum(years == y) for y in train_years + [cand]}\n",
    "    if all(counts[y] >= MIN_DAYS_PER_YEAR for y in train_years + [cand]):\n",
    "        first_valid_test_year = cand\n",
    "        print(\"First valid test year:\", first_valid_test_year)\n",
    "        break\n",
    "\n",
    "if first_valid_test_year is None:\n",
    "    raise RuntimeError(\"No calendar-aligned test year found with sufficient data. Lower MIN_DAYS_PER_YEAR or check your date range.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6dc0bbd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "\n",
    "window_results = []\n",
    "window_no = 0\n",
    "\n",
    "for test_year in tqdm(range(first_valid_test_year, max_year+1), desc=\"yearly rolling window\"):\n",
    "    window_no += 1\n",
    "    train_years = [test_year - 3, test_year - 2, test_year - 1]\n",
    "    train_idx = np.where(np.isin(years, train_years))[0]\n",
    "    test_idx  = np.where(years == test_year)[0]\n",
    "\n",
    "    if len(train_idx) < 700 or len(test_idx) < 230:\n",
    "        raise ValueError(\"The data length suggests there is insufficient data in window {}.\".format(window_no))\n",
    "\n",
    "    # prepare split\n",
    "    X_tr = X_lagged[train_idx]\n",
    "    y_tr = y_target[train_idx]\n",
    "    X_te = X_lagged[test_idx]\n",
    "    y_te = y_target[test_idx]\n",
    "\n",
    "    # tensors\n",
    "    X_tr_t = torch.tensor(X_tr, dtype=torch.float32)\n",
    "    y_tr_t = torch.tensor(y_tr.reshape(-1,1), dtype=torch.float32)\n",
    "    X_te_t = torch.tensor(X_te, dtype=torch.float32)\n",
    "\n",
    "    # save train and test date range\n",
    "    train_start_date = dates_lagged[train_idx[0]].date()\n",
    "    train_end_date   = dates_lagged[train_idx[-1]].date()\n",
    "    test_start_date  = dates_lagged[test_idx[0]].date()\n",
    "    test_end_date    = dates_lagged[test_idx[-1]].date()\n",
    "\n",
    "    # new model per window\n",
    "    model_window = SimpleLSTM(input_dim=input_variables, hidden_dim=HIDDEN_DIM, num_layers=NUM_LAYERS, dropout=DROPOUT)\n",
    "    optimizer = optim.Adam(model_window.parameters(), lr=LR)\n",
    "    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=LR_STEP_SIZE, gamma=LR_GAMMA)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    ds = TensorDataset(X_tr_t, y_tr_t)\n",
    "    data_loader = DataLoader(ds, batch_size=32, shuffle=True)  # batch size is no hyperparameter\n",
    "\n",
    "    # train\n",
    "    model_window.train()\n",
    "    for epoch in range(NUMBER_OF_EPOCHS):\n",
    "        epoch_loss = 0.0\n",
    "        for X_batch, y_batch in data_loader:\n",
    "            optimizer.zero_grad()\n",
    "            logits = model_window(X_batch)\n",
    "            loss = criterion(logits, y_batch)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            epoch_loss += loss.item() * X_batch.size(0)\n",
    "        # decrease learning rate\n",
    "        scheduler.step()\n",
    "\n",
    "    # predict for the whole test year\n",
    "    model_window.eval()\n",
    "    with torch.no_grad():\n",
    "        logits_test = model_window(X_te_t).squeeze()         # shape (TEST_DAYS,)\n",
    "        probs = torch.sigmoid(logits_test).cpu().numpy()\n",
    "        preds = (probs > CLASSIFICATION_THRESHOLD).astype(int)\n",
    "\n",
    "    window_df = pd.DataFrame({\n",
    "        'Actual_Sign': y_te,\n",
    "        'Predicted_Sign': preds,\n",
    "        'Pos_probability': probs,\n",
    "        'Train_Start': train_start_date,\n",
    "        'Train_End': train_end_date,\n",
    "        'Test_Start': test_start_date,\n",
    "        'Test_End': test_end_date\n",
    "    }, index=dates_lagged[test_idx])\n",
    "\n",
    "    window_results.append(window_df)\n",
    "\n",
    "# combine windows\n",
    "if window_results:\n",
    "    rolling_results = pd.concat(window_results).sort_index()\n",
    "    overall_acc = (rolling_results['Actual_Sign'] == rolling_results['Predicted_Sign']).mean()\n",
    "    print(f\"Rolling annual OOS accuracy: {overall_acc:.4f}\")\n",
    "    rolling_results.to_csv(\"LSTM_rolling_annual_results.csv\")\n",
    "else:\n",
    "    print(\"No windows produced - check TRAIN_DAYS/TEST_DAYS and data length.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2c911d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# count parameters\n",
    "sum(p.numel() for p in model_window.parameters() if p.requires_grad)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c4decbb",
   "metadata": {},
   "source": [
    "#parameters: \n",
    "- 1st hidden layer: 4x(#features x hidden_size + hidden_size x hidden_size) weights + hidden_size biases\n",
    "- subsequent hidden layers: 4x(hidden_size x hidden_size + hidden_size x hidden_size) + hidden_size biases\n",
    "- last layer (fully connceted layer): hidden_size + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4b6b98c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "rolling_results[\"Pos_probability\"].plot(figsize=(12,6))\n",
    "plt.title(\"Predicted Probability of Positive Return\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8125a42c",
   "metadata": {},
   "source": [
    "### Out of sample Prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90bdf812",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report, accuracy_score\n",
    "\n",
    "# Classification report\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(rolling_results['Actual_Sign'], rolling_results['Predicted_Sign']))\n",
    "print(\"Accuracy:\", accuracy_score(rolling_results['Actual_Sign'], rolling_results['Predicted_Sign']))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7369f463",
   "metadata": {},
   "source": [
    "## Evaluate results for each Year individually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fcf3cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, confusion_matrix, classification_report\n",
    "\n",
    "# rolling_results should exist from previous cell\n",
    "if 'rolling_results' not in globals():\n",
    "    raise RuntimeError(\"rolling_results not found â€” run the rolling evaluation cell first.\")\n",
    "\n",
    "# determine grouping year (prefer explicit Test_Year if present)\n",
    "if 'Test_Year' in rolling_results.columns:\n",
    "    years_idx = rolling_results['Test_Year']\n",
    "else:\n",
    "    years_idx = rolling_results.index.year\n",
    "\n",
    "summary_rows = []\n",
    "per_year_reports = {}\n",
    "\n",
    "for y in sorted(years_idx.unique()):\n",
    "    mask = (years_idx == y)\n",
    "    y_true = rolling_results.loc[mask, 'Actual_Sign'].values\n",
    "    y_pred = rolling_results.loc[mask, 'Predicted_Sign'].values\n",
    "\n",
    "    if len(y_true) == 0:\n",
    "        continue\n",
    "\n",
    "    acc = accuracy_score(y_true, y_pred)\n",
    "    prec = precision_score(y_true, y_pred, zero_division=0)\n",
    "    rec = recall_score(y_true, y_pred, zero_division=0)\n",
    "    f1 = f1_score(y_true, y_pred, zero_division=0)\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "\n",
    "    summary_rows.append({\n",
    "        'Year': int(y),\n",
    "        'N': len(y_true),\n",
    "        'Accuracy': acc,\n",
    "        'Precision 1': prec,\n",
    "        'Recall 1': rec,\n",
    "        'F1': f1,\n",
    "        'TN': int(cm[0,0]) if cm.shape == (2,2) else 0,\n",
    "        'FP': int(cm[0,1]) if cm.shape == (2,2) else 0,\n",
    "        'FN': int(cm[1,0]) if cm.shape == (2,2) else 0,\n",
    "        'TP': int(cm[1,1]) if cm.shape == (2,2) else 0\n",
    "    })\n",
    "\n",
    "    # store full sklearn report per year\n",
    "    per_year_reports[y] = classification_report(y_true, y_pred, zero_division=0, output_dict=False)\n",
    "\n",
    "summary_df = pd.DataFrame(summary_rows).sort_values('Year').set_index('Year')\n",
    "print(\"Per-year summary:\")\n",
    "print(summary_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e03a6bd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "ax = summary_df['Accuracy'].plot(kind='bar', figsize=(10,4), title='Out of Sample Accuracy by Year')\n",
    "ax.set_ylabel('Accuracy')\n",
    "plt.tight_layout()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
